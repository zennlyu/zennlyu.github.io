<!DOCTYPE html>
<html lang='en'>

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://fastly.jsdelivr.net'>
  <link rel="preconnect" href="https://fastly.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>CS229 Courses Note - zennlyu</title>

  
    <meta name="description" content="Intro本课程广泛介绍机器学习和统计模式识别。主题包括：监督学习（生成&#x2F;判别学习、参数化&#x2F;非参数化学习、神经网络、支持向量机）；无监督学习（聚类、维度缩减、核方法）；学习理论（偏差&#x2F;方差权衡，实用建议）；强化学习和自适应控制。本课程还将讨论机器学习的最新应用，如机器人控制、数据挖掘、自主导航、生物信息学、语音识别以及文本和网页数据处理。 Res 2018 年秋季学期学习大纲、资料：Machine">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229 Courses Note">
<meta property="og:url" content="http://example.com/2024/07/15/CS229-Courses-Note/index.html">
<meta property="og:site_name" content="zennlyu">
<meta property="og:description" content="Intro本课程广泛介绍机器学习和统计模式识别。主题包括：监督学习（生成&#x2F;判别学习、参数化&#x2F;非参数化学习、神经网络、支持向量机）；无监督学习（聚类、维度缩减、核方法）；学习理论（偏差&#x2F;方差权衡，实用建议）；强化学习和自适应控制。本课程还将讨论机器学习的最新应用，如机器人控制、数据挖掘、自主导航、生物信息学、语音识别以及文本和网页数据处理。 Res 2018 年秋季学期学习大纲、资料：Machine">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-07-15T03:58:26.037Z">
<meta property="article:modified_time" content="2024-07-16T03:54:23.633Z">
<meta property="article:author" content="zennlyu">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    


<header class="header">

<div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://fastly.jsdelivr.net/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://i.imgtg.com/2022/07/09/eTtVL.jpg" onerror="javascript:this.classList.add('error');this.src='https://fastly.jsdelivr.net/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">zennlyu</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">Blog</a><a class="nav-item" href="/more/">更多</a></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">TOC</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro"><span class="toc-text">Intro</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Res-2018-%E5%B9%B4%E7%A7%8B%E5%AD%A3%E5%AD%A6%E6%9C%9F"><span class="toc-text">Res 2018 年秋季学期</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0"><span class="toc-text">课程笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0"><span class="toc-text">3.0 基础知识复习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.1 有监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3.2 神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.3 无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.4 强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-text">3.5 学习理论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-text">参考</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-Learning-Terms"><span class="toc-text">Supervised Learning Terms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Housing-Example-Jump-Start"><span class="toc-text">Housing Example Jump Start</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-LMS-Algorithms"><span class="toc-text">1. LMS Algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-The-normal-equation"><span class="toc-text">2.The normal equation</span></a></li></ol></li></ol></div></div></div>


</div>


    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">Home</a><span class="sep"></span><a class="cap breadcrumb" href="/">Blog</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/machine-learning/">machine learning</a></div><div id="post-meta">Posted on&nbsp;<time datetime="2024-07-15T03:58:26.037Z">2024-07-15</time></div></div>

<article class='content md post'>
<h1 class="article-title"><span>CS229 Courses Note</span></h1>
<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>本课程广泛介绍机器学习和统计模式识别。主题包括：监督学习（生成/判别学习、参数化/非参数化学习、神经网络、支持向量机）；无监督学习（聚类、维度缩减、核方法）；学习理论（偏差/方差权衡，实用建议）；强化学习和自适应控制。本课程还将讨论机器学习的最新应用，如机器人控制、数据挖掘、自主导航、生物信息学、语音识别以及文本和网页数据处理。</p>
<h2 id="Res-2018-年秋季学期"><a href="#Res-2018-年秋季学期" class="headerlink" title="Res 2018 年秋季学期"></a>Res 2018 年秋季学期</h2><p>学习大纲、资料：<a href="https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/syllabus-autumn2018.html">Machine Learning</a></p>
<p>Youtube 视频：<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DjGwO_UgTS7I%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">https://www.youtube.com/watch?v=jGwO_UgTS7I&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU</a></p>
<p>Bilibili 视频：<a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1JE411w7Ub%3Ffrom%3Dsearch%26seid%3D9965245452780421393">【斯坦福大学】CS229 机器学习 · 2018年（完结·中英字幕·机翻）_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></p>
<h2 id="课程笔记"><a href="#课程笔记" class="headerlink" title="课程笔记"></a>课程笔记</h2><p>课程笔记根据 2018 年视频和 2020 年教学大概完成。主要分 5 大部分：基础知识复习、有监督学习、神经网络、无监督学习、强化学习、学习理论。</p>
<p>（笔记链接将持续更新）</p>
<h3 id="3-0-基础知识复习"><a href="#3-0-基础知识复习" class="headerlink" title="3.0 基础知识复习"></a>3.0 基础知识复习</h3><ul>
<li>CS229 机器学习：基础知识 线性代数和微积分</li>
<li>CS229 机器学习：基础知识 概率论</li>
<li>CS229 机器学习：基础知识 Python 和 Numpy</li>
<li>CS229 机器学习：基础知识 评价指标</li>
</ul>
<h3 id="3-1-有监督学习"><a href="#3-1-有监督学习" class="headerlink" title="3.1 有监督学习"></a>3.1 有监督学习</h3><p><strong>有监督学习：</strong>线性回归和梯度下降、逻辑回归、梯度下降、广义线性模型、高斯判别分析、朴素贝叶斯、支持向量机、核方法、决策树和集成方法</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/133536304">CS229 机器学习：有监督学习 线性回归和梯度下降</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/303634124">CS229 机器学习：有监督学习 逻辑回归和牛顿法</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/305693076">CS229 机器学习：有监督学习 广义线性模型</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/307454734">CS229 机器学习： 有监督学习 生成学习算法</a></li>
<li>CS229 机器学习：有监督学习 支持向量机</li>
<li>CS229 机器学习：有监督学习 核方法</li>
<li>CS229 机器学习：有监督学习 决策树和集成方法</li>
</ul>
<h3 id="3-2-神经网络"><a href="#3-2-神经网络" class="headerlink" title="3.2 神经网络"></a>3.2 神经网络</h3><p><strong>神经网络：</strong>（课程中应该属于有监督学习，我们暂且将其分出来，因为深度学习现在很火，而且可以作为一个通用工具）</p>
<ul>
<li>CS229 机器学习 神经网络 前向传播</li>
<li>CS229 机器学习 神经网络 反向传播</li>
<li>CS229 机器学习 神经网络 深度学习</li>
</ul>
<h3 id="3-3-无监督学习"><a href="#3-3-无监督学习" class="headerlink" title="3.3 无监督学习"></a>3.3 无监督学习</h3><p><strong>无监督学习：</strong>K 均值、高斯混合模型、期望最大算法、因子分析、主成分分析、独立成分分析</p>
<ul>
<li>CS229 机器学习 无监督学习 K 均值</li>
<li>CS229 机器学习 无监督学习 高斯混合模型和它的期望最大算法</li>
<li>CS229 机器学习 无监督学习 期望最大算法</li>
<li>CS229 机器学习 无监督学习 因子分析</li>
<li>CS229 机器学习 无监督学习 主成分分析</li>
<li>CS229 机器学习 无监督学习 独立成分分析</li>
</ul>
<h3 id="3-4-强化学习"><a href="#3-4-强化学习" class="headerlink" title="3.4 强化学习"></a>3.4 强化学习</h3><p><strong>强化学习：</strong>马尔可夫决策过程、值/策略迭代、梯度策略</p>
<ul>
<li>CS229 机器学习 强化学习马尔可夫决策过程</li>
<li>CS229 机器学习 强化学习 值迭代、策略迭代算法</li>
<li>CS229 机器学习 强化学习 线性控制系统</li>
<li>CS229 机器学习 强化学习 梯度策略</li>
</ul>
<h3 id="3-5-学习理论"><a href="#3-5-学习理论" class="headerlink" title="3.5 学习理论"></a>3.5 学习理论</h3><p><strong>学习理论：</strong>偏差/方差权衡，实用建议</p>
<ul>
<li>CS229 机器学习 学习理论 偏差/方差权衡</li>
<li>CS229 机器学习 学习理论 机器学习实用建议</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>CS229 官方讲义：<a href="https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/">CS229: Machine Learning</a></p>
<p>CS229 讲义中文翻译：<a href="https://link.zhihu.com/?target=https%3A//github.com/Kivy-CN/Stanford-CS-229-CN/">https://github.com/Kivy-CN/Stanford-CS-229-CN/</a> 、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28702753">CycleUser：斯坦福大学机器学习 CS229 课程讲义翻译完毕</a></p>
<p>口仆的学习笔记中 CS229 部分：<a target="_blank" rel="noopener" href="https://www.zhihu.com/column/xxwywzy">口仆的学习笔记</a></p>
<h2 id="Supervised-Learning-Terms"><a href="#Supervised-Learning-Terms" class="headerlink" title="Supervised Learning Terms"></a>Supervised Learning Terms</h2><ul>
<li>Regression</li>
<li>Classification</li>
<li>Terms:<ul>
<li>input <strong>Features</strong>: $x^{(i)}$</li>
<li>output <strong>Target</strong>: $y^{(i)}$</li>
<li><strong>Training sample</strong>: a pair $(x^{(i)},y^{(i)})$</li>
<li><strong>Training set</strong>: a list of $n$ training examples {$(x^{(i)},y^{(i)});i=1,…,n$}</li>
<li>$\chi$ denote the space of input values, and $\gamma$ the space of output values. In this example, $\chi$ = $\gamma$ = R.</li>
<li><strong>Hypothesis</strong>: $h: \chi \mapsto \gamma$ so that $h(x)$ is a “good” predictor for the corresponding value of y</li>
</ul>
</li>
</ul>
<h2 id="Housing-Example-Jump-Start"><a href="#Housing-Example-Jump-Start" class="headerlink" title="Housing Example Jump Start"></a>Housing Example Jump Start</h2><p>First approximate y as a linear function of x:</p>
<script type="math/tex; mode=display">
\begin{align}
h_\theta (x)=\theta_0+\theta_1x_1+\theta_2x_2
\end{align}</script><ul>
<li><p>$\theta_i’s$: <strong>Parameters/Weights</strong>, parameterizing the space of linear functions mapping from $\Chi$ to $\Upsilon$ </p>
</li>
<li><p>$h(x)$: No risk of confusion, then drop the $\theta$</p>
</li>
</ul>
<p>Let $x<em>0$ =1, that $$h(x)=\begin{align}\sum</em>{i=0}^{d}\theta_ix_i\end{align} = \theta^Tx$$ </p>
<ul>
<li>where on the right-hand side above we are viewing θ and x both as vectors, and here d is the number of input variables (not counting x0).</li>
</ul>
<p>Then how to pick parameters $\theta$ :</p>
<ul>
<li><p>make $h(x)$ close to $y$, at least for the training examples we have. </p>
</li>
<li><p>To formalize this, we will define a function that measures, for each value of the $θ’$s, how close the $h(x^{(i)})$’s are to the corresponding $y^{(i)}$’s. We define the <strong>cost function</strong>: $\begin{align} J(\theta)=\tfrac{1}{2}\sum<em>{i=1}^{n}(h</em>\theta(x^{(i)})- y^{i})^2\end{align}$</p>
</li>
<li><p><strong>ordinary least squares regression model</strong></p>
</li>
</ul>
<h3 id="1-LMS-Algorithms"><a href="#1-LMS-Algorithms" class="headerlink" title="1. LMS Algorithms"></a>1. LMS Algorithms</h3><p><strong>choose $\theta$ to minimize $J(\theta)$:</strong> search algorithm starts “initial guesses” for θ, and that repeatedly changes $θ$ to make $J(θ)$ smaller, until hopefully converge to a value of $θ$ that minimizes $J(θ)$.</p>
<p>Specifically, let’s consider the <strong>gradient descent algorithm</strong>, which starts with some initial $θ$, and repeatedly performs the update: </p>
<script type="math/tex; mode=display">
\begin{align}
\theta_j := \theta_j-\alpha\tfrac{\partial}{\partial\theta_j}J_\theta. 
\end{align}</script><p>This update is simultaneously performed for all values of $j=0,…,d$</p>
<ul>
<li>$\alpha$: Learning rate</li>
</ul>
<p>To implement the algorithm: <strong>work out what is the partial derivative term</strong> on the right hand side. </p>
<ul>
<li>Let’s first work it out for the case of if we have only one training example (x,y), so that we can neglect the sum in the definition of J. We have:</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\tfrac{\partial}{\partial\theta_j}J_\theta &= \tfrac{\partial}{\partial\theta_j}\tfrac{1}{2}(h_\theta(x)-y)^2 \\
& =2\cdot\tfrac{1}{2}(h_\theta(x)-y)\cdot\tfrac{\partial}{\partial\theta_j}(h_\theta(x)-y) \\
& = (h_\theta(x)-y)\cdot\tfrac{\partial}{\partial\theta_j}(\sum_{i=0}^{d}\theta_ix_i-y) \\
& = (h_\theta(x)-y)\cdot x_j
\end{aligned}</script><p>For a single training example, this gives the update rule:1</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_{j}</script><p>The rule : <strong>LMS update rule</strong> (“least mean squares”), and is also known as the <strong>Widrow-Hoff learning rule</strong>. </p>
<ul>
<li>This rule has several properties that seem natural and intuitive. </li>
<li>For instance, the magnitude of the update is proportional to the error term (y(i) − hθ(x(i))); thus, for instance, if we are encountering a training example on which our prediction nearly matches the actual value of y(i), then we find that there is little need to change the parameters; </li>
<li>in contrast, a larger change to the parameters will be made if our prediction hθ(x(i)) has a large error (i.e., if it is very far from y(i)).</li>
</ul>
<p>Derived the LMS rule:  only a single training example.  There are two ways to modify this method for a training set of more than one example.</p>
<p><strong>1st</strong> <strong>batch gradient descent</strong>. </p>
<ul>
<li>_ replace it with the following algorithm: Repeat Until Convergence —-</li>
</ul>
<script type="math/tex; mode=display">
Repeat\: Until\: Convergence - \{
\theta_j := \theta_j+\alpha\sum_{i=1}^{n}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)},(for\:every\:j) 
\}</script><ul>
<li>By grouping the updates of the coordinates into an update of the vector θ, rewrite:</li>
</ul>
<script type="math/tex; mode=display">
\theta := \theta+\alpha\sum_{i=1}^{n}(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}</script><ul>
<li>can easily verify the quantity in the summation in the update rule above is just $\tfrac{∂J(θ)}{∂θ_j}$(for the original definition of $J$). So, this is simply gradient descent on the original cost function$J$. This method looks at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong>. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here</li>
</ul>
<p><strong>2nd stochastic gradient descent / incremental gradient descent</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
Loop\: \{ for\: i=1\: to\: n \{\\
\theta_j := \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)} \\
(for\:every\:j)  \} \\
\}
\end{aligned}</script><p>By grouping the updates of the coordinates into an update of the vector θ, we can rewrite update (2) in a slightly more succinct way:</p>
<script type="math/tex; mode=display">
\theta := \theta+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}</script><h3 id="2-The-normal-equation"><a href="#2-The-normal-equation" class="headerlink" title="2.The normal equation"></a>2.The normal equation</h3>

<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>License</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>READ NEXT</span></section><section class="body fs14"><a id="next" href="/2024/07/15/6.828%20xv6-book/">S081 xv6-book<span class="note">Older</span></a><div class="line"></div><a id="prev" href="/2024/07/15/6.828-Utilities/">S081 Labs<span class="note">Newer</span></a></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</p>
<p>This site was deployed by <a href="http://example.com/">@zennlyu</a> using <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.8.0" title="v1.8.0">Stellar</a>.</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.8.0';
  stellar.config = {
    date_suffix: {
      just: 'Just',
      min: 'minutes ago',
      hour: 'hours ago',
      day: 'days ago',
      month: 'months ago',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://fastly.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://fastly.jsdelivr.net/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://fastly.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://fastly.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://fastly.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://fastly.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://fastly.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
