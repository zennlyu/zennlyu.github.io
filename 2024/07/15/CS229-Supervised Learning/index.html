<!DOCTYPE html>
<html lang='en'>

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://fastly.jsdelivr.net'>
  <link rel="preconnect" href="https://fastly.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>CS229-Supervised Learning - zennlyu</title>

  
    <meta name="description" content="Supervised Learning Terms Regression Classification Terms: input Features: $x^{(i)}$ output Target: $y^{(i)}$ Training sample: a pair $(x^{(i)},y^{(i)})$ Training set: a list of $n$ training examples">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229-Supervised Learning">
<meta property="og:url" content="http://example.com/2024/07/15/CS229-Supervised%20Learning/index.html">
<meta property="og:site_name" content="zennlyu">
<meta property="og:description" content="Supervised Learning Terms Regression Classification Terms: input Features: $x^{(i)}$ output Target: $y^{(i)}$ Training sample: a pair $(x^{(i)},y^{(i)})$ Training set: a list of $n$ training examples">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-07-15T03:58:26.037Z">
<meta property="article:modified_time" content="2024-07-15T03:58:26.038Z">
<meta property="article:author" content="zennlyu">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    


<header class="header">

<div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://fastly.jsdelivr.net/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://i.imgtg.com/2022/07/09/eTtVL.jpg" onerror="javascript:this.classList.add('error');this.src='https://fastly.jsdelivr.net/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">zennlyu</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">Blog</a><a class="nav-item" href="/more/">更多</a></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">TOC</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-Learning-Terms"><span class="toc-text">Supervised Learning Terms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Housing-Example-Jump-Start"><span class="toc-text">Housing Example Jump Start</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-LMS-Algorithms"><span class="toc-text">1. LMS Algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-The-normal-equation"><span class="toc-text">2.The normal equation</span></a></li></ol></li></ol></div></div></div>


</div>


    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">Home</a><span class="sep"></span><a class="cap breadcrumb" href="/">Blog</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/machine-learning/">machine learning</a></div><div id="post-meta">Posted on&nbsp;<time datetime="2024-07-15T03:58:26.037Z">2024-07-15</time></div></div>

<article class='content md post'>
<h1 class="article-title"><span>CS229-Supervised Learning</span></h1>
<h2 id="Supervised-Learning-Terms"><a href="#Supervised-Learning-Terms" class="headerlink" title="Supervised Learning Terms"></a>Supervised Learning Terms</h2><ul>
<li>Regression</li>
<li>Classification</li>
<li>Terms:<ul>
<li>input <strong>Features</strong>: $x^{(i)}$</li>
<li>output <strong>Target</strong>: $y^{(i)}$</li>
<li><strong>Training sample</strong>: a pair $(x^{(i)},y^{(i)})$</li>
<li><strong>Training set</strong>: a list of $n$ training examples {$(x^{(i)},y^{(i)});i&#x3D;1,…,n$}</li>
<li>$\chi$ denote the space of input values, and $\gamma$ the space of output values. In this example, $\chi$ &#x3D; $\gamma$ &#x3D; R.</li>
<li><strong>Hypothesis</strong>: $h: \chi \mapsto \gamma$ so that $h(x)$ is a “good” predictor for the corresponding value of y</li>
</ul>
</li>
</ul>
<h2 id="Housing-Example-Jump-Start"><a href="#Housing-Example-Jump-Start" class="headerlink" title="Housing Example Jump Start"></a>Housing Example Jump Start</h2><p>First approximate y as a linear function of x:<br>$$<br>\begin{align}<br>h_\theta (x)&#x3D;\theta_0+\theta_1x_1+\theta_2x_2<br>\end{align}<br>$$</p>
<ul>
<li><p>$\theta_i’s$: <strong>Parameters&#x2F;Weights</strong>, parameterizing the space of linear functions mapping from $\Chi$ to $\Upsilon$ </p>
</li>
<li><p>$h(x)$: No risk of confusion, then drop the $\theta$</p>
</li>
</ul>
<p>Let $x_0$ &#x3D;1, that $$h(x)&#x3D;\begin{align}\sum_{i&#x3D;0}^{d}\theta_ix_i\end{align} &#x3D; \theta^Tx$$ </p>
<ul>
<li>where on the right-hand side above we are viewing θ and x both as vectors, and here d is the number of input variables (not counting x0).</li>
</ul>
<p>Then how to pick parameters $\theta$ :</p>
<ul>
<li><p>make $h(x)$ close to $y$, at least for the training examples we have. </p>
</li>
<li><p>To formalize this, we will define a function that measures, for each value of the $θ’$s, how close the $h(x^{(i)})$’s are to the corresponding $y^{(i)}$’s. We define the <strong>cost function</strong>: $\begin{align} J(\theta)&#x3D;\tfrac{1}{2}\sum_{i&#x3D;1}^{n}(h_\theta(x^{(i)})- y^{i})^2\end{align}$</p>
</li>
<li><p><strong>ordinary least squares regression model</strong></p>
</li>
</ul>
<h3 id="1-LMS-Algorithms"><a href="#1-LMS-Algorithms" class="headerlink" title="1. LMS Algorithms"></a>1. LMS Algorithms</h3><p><strong>choose $\theta$ to minimize $J(\theta)$:</strong> search algorithm starts “initial guesses” for θ, and that repeatedly changes $θ$ to make $J(θ)$ smaller, until hopefully converge to a value of $θ$ that minimizes $J(θ)$.</p>
<p>Specifically, let’s consider the <strong>gradient descent algorithm</strong>, which starts with some initial $θ$, and repeatedly performs the update:<br>$$<br>\begin{align}<br>\theta_j :&#x3D; \theta_j-\alpha\tfrac{\partial}{\partial\theta_j}J_\theta.<br>\end{align}<br>$$</p>
<p>This update is simultaneously performed for all values of $j&#x3D;0,…,d$</p>
<ul>
<li>$\alpha$: Learning rate</li>
</ul>
<p>To implement the algorithm: <strong>work out what is the partial derivative term</strong> on the right hand side. </p>
<ul>
<li>Let’s first work it out for the case of if we have only one training example (x,y), so that we can neglect the sum in the definition of J. We have:</li>
</ul>
<p>$$<br>\begin{aligned}<br>\tfrac{\partial}{\partial\theta_j}J_\theta &amp;&#x3D; \tfrac{\partial}{\partial\theta_j}\tfrac{1}{2}(h_\theta(x)-y)^2 \<br>&amp; &#x3D;2\cdot\tfrac{1}{2}(h_\theta(x)-y)\cdot\tfrac{\partial}{\partial\theta_j}(h_\theta(x)-y) \<br>&amp; &#x3D; (h_\theta(x)-y)\cdot\tfrac{\partial}{\partial\theta_j}(\sum_{i&#x3D;0}^{d}\theta_ix_i-y) \<br>&amp; &#x3D; (h_\theta(x)-y)\cdot x_j<br>\end{aligned}<br>$$</p>
<p>For a single training example, this gives the update rule:1<br>$$<br>\theta_j :&#x3D; \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_{j}<br>$$<br>The rule : <strong>LMS update rule</strong> (“least mean squares”), and is also known as the <strong>Widrow-Hoff learning rule</strong>. </p>
<ul>
<li>This rule has several properties that seem natural and intuitive. </li>
<li>For instance, the magnitude of the update is proportional to the error term (y(i) − hθ(x(i))); thus, for instance, if we are encountering a training example on which our prediction nearly matches the actual value of y(i), then we find that there is little need to change the parameters; </li>
<li>in contrast, a larger change to the parameters will be made if our prediction hθ(x(i)) has a large error (i.e., if it is very far from y(i)).</li>
</ul>
<p>Derived the LMS rule:  only a single training example.  There are two ways to modify this method for a training set of more than one example.</p>
<p><strong>1st</strong> <strong>batch gradient descent</strong>. </p>
<ul>
<li>_ replace it with the following algorithm: Repeat Until Convergence —</li>
</ul>
<p>$$<br>Repeat: Until: Convergence - {<br>\theta_j :&#x3D; \theta_j+\alpha\sum_{i&#x3D;1}^{n}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)},(for:every:j)<br>}<br>$$</p>
<ul>
<li>By grouping the updates of the coordinates into an update of the vector θ, rewrite:</li>
</ul>
<p>$$<br>\theta :&#x3D; \theta+\alpha\sum_{i&#x3D;1}^{n}(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}<br>$$</p>
<ul>
<li>can easily verify the quantity in the summation in the update rule above is just $\tfrac{∂J(θ)}{∂θ_j}$(for the original definition of $J$). So, this is simply gradient descent on the original cost function$J$. This method looks at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong>. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here</li>
</ul>
<p><strong>2nd stochastic gradient descent &#x2F; incremental gradient descent</strong><br>$$<br>\begin{aligned}<br>Loop: { for: i&#x3D;1: to: n {\<br>\theta_j :&#x3D; \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)} \<br>(for:every:j)  } \<br>}<br>\end{aligned}<br>$$<br>By grouping the updates of the coordinates into an update of the vector θ, we can rewrite update (2) in a slightly more succinct way:<br>$$<br>\theta :&#x3D; \theta+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}<br>$$</p>
<h3 id="2-The-normal-equation"><a href="#2-The-normal-equation" class="headerlink" title="2.The normal equation"></a>2.The normal equation</h3>

<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>License</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>READ NEXT</span></section><section class="body fs14"><a id="next" href="/2024/07/15/CS229-Courses-Note/">CS229 Courses Note<span class="note">Older</span></a><div class="line"></div><a id="prev" href="/2024/07/15/alg-ANALYSIS%20OF%20ALGORITHMS/">coursera 1.4 ANALYSIS OF ALGORITHMS<span class="note">Newer</span></a></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</p>
<p>This site was deployed by <a href="http://example.com/">@zennlyu</a> using <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.8.0" title="v1.8.0">Stellar</a>.</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.8.0';
  stellar.config = {
    date_suffix: {
      just: 'Just',
      min: 'minutes ago',
      hour: 'hours ago',
      day: 'days ago',
      month: 'months ago',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://fastly.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://fastly.jsdelivr.net/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://fastly.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://fastly.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://fastly.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://fastly.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://fastly.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
