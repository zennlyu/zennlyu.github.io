---
title: 6.824 W1 MapReduce, RPC & threads
categories: [distributed system]
tags: [6.824]

---

# MapReduce

## Lecture 1

### Labs

- Lab 1: MapReduce
- Lab 2: replication for fault-tolerance using Raft
- Lab 3: fault-tolerant key/value store
- Lab 4: sharded key/value store

### Main Topics

This is a course about infrastructure for applications.

  * Storage.
  * Communication.
  * Computation.

#### Topic: implementation

-  RPC, threads, concurrency control.

#### Topic: performance

1. The goal: scalable throughput
2. Nx servers -> Nx total throughput via parallel CPU, disk, net.
3. [diagram: users, application servers, storage servers]
4. So handling more load only requires buying more computers rather than re-design by expensive programmers.
5. Effective when you can divide work w/o much interaction.

#### Topic: fault tolerance

We often want:

- **Availability** -- app can make progress despite failures
- **Recoverability** -- app will come back to life when failures are repaired

Big idea: replicated servers.

- If one server crashes, can proceed using the other(s).
- Labs 1, 2 and 3

#### Topic: consistency

General-purpose infrastructure needs well-defined behavior.

> E.g. "`Get(k)` yields the value from the most recent `Put(k,v)`."

$$
\begin{cases} 
Put(k,v) \\
Get(k) 
\end{cases}
$$

Achieving good behavior is hard!

- "Replica" servers are hard to keep identical.
- Clients may crash midway through multi-step update.
- Servers may crash, e.g. after executing but before replying.
- Network partition may make live servers look dead; risk of "split brain".

Consistency and performance are enemies.

- Strong consistency requires communication,

  > e.g. `Get()` must check for a recent `Put()`.

- Many designs provide only **weak consistency, to gain speed.**

e.g. `Get()` does *not* yield the latest `Put()`!

Painful for application programmers but may be a good trade-off.

Many design points are possible in the consistency/performance spectrum!

### CASE STUDY: MapReduce

#### MapReduce overview

context: multi-hour computations on multi-terabyte data-sets

> e.g. build search index, or sort, or analyze structure of web

- only practical with 1000s of computers
- applications not written by distributed systems experts

 overall goal: easy for non-specialist programmers

 programmer just defines Map and Reduce functions: often fairly simple sequential code

MR takes care of, and hides, all aspects of distribution!

#### Abstract view of a MapReduce job

input is (already) split into M files

```
Input1 -> Map -> a,1 b,1
Input2 -> Map ->b,1
Input3 -> Map -> a,1c,1
            |||
            ||-> Reduce -> c,1
            |-----> Reduce -> b,2
            ---------> Reduce -> a,2
```

MR calls `Map()` for each input file, produces set of $<k2,v2>$

- "intermediate" data
- each `Map()` call is a "task"

MR gathers all intermediate $v2$'s for a given $k2$, and passes each key + values to a Reduce call

final output is set of $<k2,v3>$ pairs from `Reduce()`s

Example: word count : input is thousands of text files

```c
Map(k, v)
split v into words
for each word w
 emit(w, "1")
Reduce(k, v)
	emit(len(v))
```

#### MapReduce scales well:

$N$ "worker" computers get you $Nx$ throughput.

- Maps()s can run in parallel, since they don't interact.
- Same for Reduce()s.

So you can get more throughput by buying more computers.

#### MapReduce hides many details:

- sending app code to servers
- tracking which tasks are done
- moving data from Maps to Reduces
- balancing load over servers
- recovering from failures

#### However, MapReduce limits what apps can do:

- No interaction or state (other than via intermediate output).
- No iteration, no multi-stage pipelines.
- No real-time or streaming processing.

#### Input and output are stored on the GFS cluster file system

MR needs huge parallel input and output throughput.

GFS splits files over many servers, in 64 MB chunks

- Maps read in parallel
- Reduces write in parallel

GFS also replicates each file on 2 or 3 servers

Having GFS is a big win for MapReduce

#### What will likely limit the performance?

We care since that's the thing to optimize. CPU? memory? disk? network?

In 2004 authors were limited by network capacity —— What does MR send over the network?

- Maps read input from GFS.
- Reduces read Map output ：Can be as large as input, e.g. for sorting.
- Reduces write output files to GFS.

[diagram: servers, tree of network switches]

In MR's all-to-all shuffle, half of traffic goes through root switch.

Paper's root switch: 100 to 200 gigabits/second, total

-  1800 machines, so 55 megabits/second/machine.
-  55 is small, e.g. much less than disk or RAM speed.

Today: networks and root switches are much faster relative to CPU/disk.

#### Some details (paper's Figure 1):

one master, that hands out tasks to workers and remembers progress.

1. master gives Map tasks to workers until all Maps complete

   Maps write output (intermediate data) to local disk

   Maps split output, by hash, into one file per Reduce task

  2. after all Maps have finished, master hands out Reduce tasks

     each Reduce fetches its intermediate output from (all) Map workers

     each Reduce task writes a separate output file on GFS

#### How does MR minimize network use?

Master tries to run each Map task on GFS server that stores its input.

- All computers run both GFS and MR workers
- So input is read from local disk (via GFS), not over network.

Intermediate data goes over network just once.

- Map worker writes to local disk.
- Reduce workers read directly from Map workers, not via GFS.

Intermediate data partitioned into files holding many keys.

- R is much smaller than the number of keys.
- Big network transfers are more efficient.

#### How does MR get good load balance?

Wasteful and slow if N-1 servers have to wait for 1 slow server to finish.

But some tasks likely take longer than others.

Solution: many more tasks than workers.

-  Master hands out new tasks to workers who finish previous tasks.
-  So no task is so big it dominates completion time (hopefully).
-  So faster servers do more tasks than slower ones, finish abt the same time.

#### What about fault tolerance?

I.e. what if a worker crashes during a MR job?

We want to completely hide failures from the application programmer!

Does MR have to re-run the whole job from the beginning? Why not?

- MR re-runs just the failed Map()s and Reduce()s.
- Suppose MR runs a Map twice, one Reduce sees first run's output, —— another Reduce sees the second run's output?
- Correctness requires re-execution to yield exactly the same output.
- So Map and Reduce must be pure deterministic functions:
  - they are only allowed to look at their arguments.
  - no state, no file I/O, no interaction, no external communication.

What if you wanted to allow non-functional Map or Reduce?

- Worker failure would require whole job to be re-executed, or you'd need to create synchronized global checkpoints.

#### Details of worker crash recovery:

* Map worker crashes:
  * master notices worker no longer responds to pings
  * master knows which Map tasks it ran on that worker 
    * those tasks' intermediate output is now lost, must be re-created
    * master tells other workers to run those tasks
  * can omit re-running if Reduces already fetched the intermediate data
* Reduce worker crashes.
  * finished tasks are OK -- stored in GFS, with replicas.
  * master re-starts worker's unfinished tasks on other workers.

#### Other failures/problems:

  * What if the master gives two workers the same Map() task?
    * perhaps the master incorrectly thinks one worker died.
    * it will tell Reduce workers about only one of them.
  * What if the master gives two workers the same Reduce() task?
    * they will both try to write the same output file on GFS!
    * atomic GFS rename prevents mixing; one complete file will be visible.
  * What if a single worker is very slow -- a "straggler"?
    * perhaps due to flakey hardware.
    * master starts a second copy of last few tasks.
  * What if a worker computes incorrect output, due to broken h/w or s/w?
    * too bad! MR assumes "fail-stop" CPUs and software.
  * What if the master crashes?

#### Current status?

- Hugely influential (Hadoop, Spark, &c).
- Probably no longer in use at Google.
  - Replaced by Flume / FlumeJava (see paper by Chambers et al).
  - GFS replaced by Colossus (no good description), and BigTable.

#### Conclusion

MapReduce single-handedly made big cluster computation popular.

“-” Not the most efficient or flexible.

“+” Scales well.

“+” Easy to program -- failures and data movement are hidden.

These were good trade-offs in practice.

## Lecture 2: Infrastructure: RPC and threads

### Why Go?

- good support for threads
- convenient RPC
- type- and memory- safe
- garbage-collected (no use after freeing problems)
- threads + GC is particularly attractive!
- not too complex

After the tutorial, use https://golang.org/doc/effective_go.html

### Threads

- a useful structuring tool, but can be tricky
- Go calls them goroutines; everyone else calls them threads

### Thread = "thread of execution"

- threads allow one program to do many things at once
- each thread executes serially, just like an ordinary non-threaded program
- the threads share memory
- each thread includes some per-thread state:
  - program counter,
  - registers, 
  - stack,
  - what it's waiting for

### Why threads?

#### I/O concurrency

- Client sends requests to many servers in parallel and waits for replies.
- Server processes multiple client requests; each request may block.
- While waiting for the disk to read data for client X, 
  - process a request from client Y.


#### Multicore performance

- Execute code in parallel on several cores.

#### Convenience

- In background, once per second, check whether each worker is still alive.

### Is there an alternative to threads?

#### "event-driven."

- write code that explicitly interleaves activities, in a single thread. Usually called "event-driven."
- Keep a table of state about each activity, e.g. each client request.
- One "event" loop that:
  - checks for new input for each activity (e.g. arrival of reply from server),
  - does the next step for each activity,
  - updates state.

- Event-driven gets you I/O concurrency, and eliminates thread costs (which can be substantial), but doesn't get multi-core speedup, and is painful to program.

### Threading challenges:

#### sharing data safely

- what if two threads do n = n + 1 at the same time?
  - or one thread reads while another increments?

- this is a "race" -- and is often a bug
- -> use locks (Go's sync.Mutex)
- -> or avoid sharing mutable data

#### coordination between threads

- one thread is producing data, another thread is consuming it
  - how can the consumer wait (and release the CPU)?
  - how can the producer wake up the consumer?

- -> use Go channels or sync.Cond or sync.WaitGroup

#### deadlock

cycles via locks and/or communication (e.g. RPC or Go channels)



Let's look at the tutorial's web crawler as a threading example.

### What is a web crawler?

-   goal: fetch all web pages, e.g. to feed to an indexer
-   you give it a starting web page
-   it recursively follows all links
-   but don't fetch a given page more than once
    -   and don't get stuck in cycles


### Crawler challenges

#### Exploit I/O concurrency

- Network latency is more limiting than network capacity
- Fetch many URLs at the same time
  - To increase URLs fetched per second
- => Use threads for concurrency

#### Fetch each URL only *once*

- avoid wasting network bandwidth
- be nice to remote servers
- => Need to remember which URLs visited 

#### Know when finished



We'll look at three styles of solution [crawler.go on schedule page]

### Serial crawler

- performs depth-first exploration via recursive Serial calls 
- the "fetched" map avoids repeats, breaks cycles
  - a single map, passed by reference, caller sees callee's updates
- but: fetches only one page at a time -- slow
  - can we just put a "go" in front of the Serial() call?
  - let's try it... what happened?

### Concurrent Mutex crawler

- Creates a thread for each page fetch
  - Many concurrent fetches, higher fetch rate
- the "go func" creates a goroutine and starts it running
  - func... is an "anonymous function"
- The threads share the "fetched" map
  - So only one thread will fetch any given page
- Why the Mutex (Lock() and Unlock())?
  - One reason:
    - Two threads make simultaneous calls to ConcurrentMutex() with same URL
      - Due to two different pages containing link to same URL
    - T1 reads fetched[url], T2 reads fetched[url]
    - Both see that url hasn't been fetched (already == false)
    - Both fetch, which is wrong
    - The mutex causes one to wait while the other does both check and set
      - So only one thread sees already==false
    - We say "the lock protects the data"
      - But not Go does not enforce any relationship between locks and data!
    - The code between lock/unlock is often called a "critical section"
  - Another reason:
    - Internally, map is a complex data structure (tree? expandable hash?)
    - Concurrent update/update may wreck internal invariants
    - Concurrent update/read may crash the read
  - What if I comment out Lock() / Unlock()?
    - go run crawler.go
      - Why does it work?
    - go run -race crawler.go
      - Detects races even when output is correct!
- How does the ConcurrentMutex crawler decide it is done?
  - sync.WaitGroup
  - Wait() waits for all Add()s to be balanced by Done()s
    - i.e. waits for all child threads to finish
  - [diagram: tree of goroutines, overlaid on cyclic URL graph]
  - there's a WaitGroup per node in the tree
- How many concurrent threads might this crawler create?

### ConcurrentMutex crawler

#### a Go channel:

- a channel is an object

  ```go
  ch := make(chan int)
  ```

- a channel lets one thread send an object to another thread

  ```go
  ch <- x
  ```

the sender waits until some goroutine receives, a receiver waits until some goroutine sends

```go
y := <- ch
for y := range ch
```

- channels both communicate and synchronize
- several threads can send and receive on a channel
- channels are cheap
- remember: sender blocks until the receiver receives!
  - "synchronous"
  - watch out for deadlock

#### ConcurrentChannel coordinator()

- coordinator() creates a worker goroutine to fetch each page
- woker() sends slice of page's URLs on a channel
  - multiple workers send on the single channel
- coordinator() reads URL slices from the channel

#### At what line does the coordinator wait?

- Does the coordinator use CPU time while it waits?

#### Note: there is no recursion here; instead there's a work list.

#### Note: no need to lock the fetched map, because it isn't shared!

#### How does the coordinator know it is done?

- Keeps count of workers in n.
- Each worker sends exactly one item on channel.



Why is it safe for multiple threads use the same channel?

Worker thread writes url slice, coordinator reads it, is that a race?

  * worker only writes slice *before* sending
  * coordinator only reads slice *after* receiving
    So they can't use the slice at the same time.

Why does ConcurrentChannel() create a goroutine just for "ch <- ..."?

- Let's get rid of the goroutine...

When to use sharing and locks, versus channels?

- Most problems can be solved in either style
- What makes the most sense depends on how the programmer thinks
  - state -- sharing and locks
  - communication -- channels
- For the 6.824 labs, I recommend sharing+locks for state,
  - and `sync.Cond` or channels or `time.Sleep()` for waiting/notification.

### Remote Procedure Call (RPC)

- a key piece of distributed system machinery; all the labs use RPC
- goal: easy-to-program client/server communication
- hide details of network protocols
- convert data (strings, arrays, maps, &c) to "wire format"
- portability / interoperability

### RPC message diagram:

| Client  |      | Server   |
| ------- | ---- | -------- |
| request | ---> |          |
|         | <--- | response |

### Software structure

| client app |        | handler fns |
| ---------- | ------ | ----------- |
| stub fns   | --->   | dispatcher  |
| RPC lib    |        | RPC lib     |
| net        | ------ | net         |

### Go example: kv.go on schedule page

- A toy key/value storage server -- Put(key,value), Get(key)->value
- Uses Go's RPC library
- Common:
  - Declare Args and Reply struct for each server handler.
- Client:
  - connect()'s Dial() creates a TCP connection to the server
  - get() and put() are client "stubs"
  - Call() asks the RPC library to perform the call
    - you specify server function name, arguments, place to put reply
    - library marshalls args, sends request, waits, unmarshalls reply
    - return value from Call() indicates whether it got a reply
    - usually you'll also have a reply.Err indicating service-level failure
- Server:
  - Go requires server to declare an object with methods as RPC handlers
  - Server then registers that object with the RPC library
  - Server accepts TCP connections, gives them to RPC library
  - The RPC library
    - reads each request
    - creates a new goroutine for this request
    - unmarshalls request
    - looks up the named object (in table create by Register())
    - calls the object's named method (dispatch)
    - marshalls reply
    - writes reply on TCP connection
- The server's Get() and Put() handlers
  - Must lock, since RPC library creates a new goroutine for each request
  - read args; modify reply

### A few details:

- Binding: how does client know what server computer to talk to?
  - For Go's RPC, server name/port is an argument to Dial
  - Big systems have some kind of name or configuration server
- Marshalling: format data into packets
  - Go's RPC library can pass strings, arrays, objects, maps, &c
  - Go passes pointers by copying the pointed-to data
  - Cannot pass channels or functions

### RPC problem: what to do about failures?

e.g. lost packet, broken network, slow server, crashed server

### What does a failure look like to the client RPC library?

- Client never sees a response from the server
- Client does *not* know if the server saw the request!
  - [diagram of losses at various points]
  - Maybe server never saw the request
  - Maybe server executed, crashed just before sending reply
  - Maybe server executed, but network died just before delivering reply

### Simplest failure-handling scheme: "best-effort RPC"

- `Call()` waits for response for a while
- If none arrives, re-send the request
- Do this a few times
- Then give up and return an error

### Q: is "best effort" easy for applications to cope with?

### A particularly bad situation:

- client executes
  - `Put("k", 10)`;
  - `Put("k", 20)`;
- both succeed
- what will Get("k") yield?
- [diagram, timeout, re-send, original arrives late]

### Q: is best effort ever OK?

-  read-only operations
-  operations that do nothing if repeated
   -  e.g. DB checks if record has already been inserted

### Better RPC behavior: "at-most-once RPC"

- idea: client re-sends if no answer;

  - server RPC code detects duplicate requests,
  - returns previous reply instead of re-running handler

- Q: how to detect a duplicate request?

- client includes unique ID (XID) with each request

  - uses same XID for re-send

- server:

  ```go
  if seen[xid]:
  r = old[xid]
  else
  r = handler()
  old[xid] = r
  seen[xid] = true
  ```

### some at-most-once complexities

- this will come up in lab 3
- what if two clients use the same XID?
  - big random number?
- how to avoid a huge seen[xid] table?
  - idea:
    - each client has a unique ID (perhaps a big random number)
    - per-client RPC sequence numbers
    - client includes "seen all replies <= X" with every RPC
    - much like TCP sequence #s and acks
  - then server can keep O(# clients) state, rather than O(# XIDs)
- server must eventually discard info about old RPCs or old clients
  - when is discard safe?
- how to handle dup req while original is still executing?
  - server doesn't know reply yet
  - idea: "pending" flag per executing RPC; wait or ignore

### What if an at-most-once server crashes and re-starts?

- if at-most-once duplicate info in memory, server will forget
- and accept duplicate requests after re-start
- maybe it should write the duplicate info to disk
- maybe replica server should also replicate duplicate info

### Go RPC is a simple form of "at-most-once"

- open TCP connection
- write request to TCP connection
- Go RPC never re-sends a request
  - So server won't see duplicate requests
- Go RPC code returns an error if it doesn't get a reply
  - perhaps after a timeout (from TCP)
  - perhaps server didn't see request
  - perhaps server processed request but server/net failed before reply came back

### What about "exactly once"?

unbounded retries plus duplicate detection plus fault-tolerant service
Lab 3



## MapReduce Paper Read

### 0 Abstract

Users specify 

- a map function that processes a key/value pair to generate a set of intermediate key/value pairs, 
- a reduce function that merges all intermediate values associated with the same intermediate key. 

Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. 

**Run-time system** takes care of the details of 

- partitioning the input data, 
- scheduling the program’s execution across a set of machines, 
- handling machine failures,
- managing the required intermachine communication. 

### 1 Introduction

| Section | Content                                                      |
| ------- | ------------------------------------------------------------ |
| 2       | basic programming model and gives several examples.          |
| 3       | an implementation of the MapReduce interface tailored towards our cluster-based computing environment. |
| 4       | several refinements of the programming model that we have found useful. |
| 5       | has performance measurements of our implementation for a variety of tasks. |
| 6       | explores the use of MapReduce within Google including our experiences in using it as the basis for a rewrite of our production indexing system. |
| 7       | discusses related and future work.                           |

### 2 Programming Model

#### 2.1 Counting Word

```c
map(String key, String value):
  // key: document name
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, “1”);

reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```

> - The `map` function emits each word plus an associated count of occurrences (just ‘1’ in this simple example). 
> - The `reduce` function sums together all counts emitted for a particular word.



#### 2.2 Types

$map(k1,v1)→list(k2,v2)$

$map(k2,list(v2))→list(v2)$

#### 2.3 More Examples

- **Distributed Grep**: The map function emits a line if it matches a supplied pattern. The reduce function is an identity function that just copies the supplied intermedi- ate data to the output.
- **Count of URL Access Frequency**: The map function processes logs of web page requests and outputs $⟨URL, 1⟩$. The reduce function adds together all values for the same URL and emits a $⟨URL, total count⟩$ pair.
- **Reverse Web-Link Graph**: The map function outputs $⟨target,source⟩$ pairs for each link to a target URL found in a page named source. The reduce function concatenates the list of all source URLs as- sociated with a given target URL and emits the pair: $⟨target, list(source)⟩$
- **Term-Vector per Host**: A term vector summarizes the most important words that occur in a document or a set of documents as a list of $⟨word,frequency⟩$ pairs. The map function emits a $⟨hostname, term vector⟩$ pair for each input document (where the hostname is extracted from the URL of the document). The reduce function is passed all per-document term vectors for a given host. It adds these term vectors together, throwing away infrequent terms, and then emits a final $⟨hostname, term vector⟩$ pair.
- **Inverted Index**: The map function parses each docu- ment, and emits a sequence of $⟨word, document ID⟩$ pairs. The reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a $⟨word,list(document ID)⟩$ pair.Thesetofalloutput pairs forms a simple inverted index. It is easy to augment this computation to keep track of word positions.
- **Distributed Sort**: The map function extracts the key from each record, and emits a $⟨key, record⟩$ pair. The reduce function emits all pairs unchanged. This computation depends on the partitioning facilities described in Section 4.1 and the ordering properties described in Section 4.2.

### 3 Implementation

#### 3.1 Execution Overview

![execution](execution.jpg)

首先，用户通过 MapReduce 客户端指定 Map 函数和 Reduce 函数，以及此次 MapReduce 计算的配置，包括中间结果键值对的 Partition 数量 $R$ 以及用于切分中间结果的哈希函数 $hash$ 。
用户开始 MapReduce 计算后，整个 MapReduce 计算的流程可总结如下：

1. 作为输入的文件会被分为  $M$  个 Split，每个 Split 的大小通常在 16~64 MB 之间
2. 如此，整个 MapReduce 计算包含  $M$  个Map 任务和  $R$  个 Reduce 任务。Master 结点会从空闲的 Worker 结点中进行选取并为其分配 Map 任务和 Reduce 任务
3. 收到 Map 任务的 Worker 们（又称 Mapper）开始读入自己对应的 Split，将读入的内容解析为输入键值对并调用由用户定义的 Map 函数。由 Map 函数产生的中间结果键值对会被暂时存放在缓冲内存区中
4. 在 Map 阶段进行的同时，Mapper 们周期性地将放置在缓冲区中的中间结果存入到自己的本地磁盘中，同时根据用户指定的 Partition 函数（默认为 $hash(key){\,}mod{\,}R$)。 将产生的中间结果分为 $R$ 个部分。任务完成时，Mapper 便会将中间结果在其本地磁盘上的存放位置报告给 Master
5. Mapper 上报的中间结果存放位置会被 Master 转发给 Reducer。当 Reducer 接收到这些信息后便会通过 RPC 读取存储在 Mapper 本地磁盘上属于对应 Partition 的中间结果。在读取完毕后，Reducer 会对读取到的数据进行排序以令拥有相同键的键值对能够连续分布
6. 之后，Reducer 会为每个键收集与其关联的值的集合，并以之调用用户定义的 Reduce 函数。Reduce 函数的结果会被放入到对应的 Reduce Partition 结果文件

实际上，在一个 MapReduce 集群中，Master 会记录每一个 Map 和 Reduce 任务的当前完成状态，以及所分配的 Worker。除此之外，Master 还负责将 Mapper 产生的中间结果文件的位置和大小转发给 Reducer。

值得注意的是，每次 MapReduce 任务执行时， $M$  和 $R$  的值都应比集群中的 Worker 数量要高得多，以达成集群内负载均衡的效果。

#### 3.2 Master Data Structure

**MASTER** keeps several data structures. For each map task and reduce task, it stores 

- the state (idle, in-progress, or completed),
- the identity of the worker machine (for non-idle tasks).

The master is the conduit through which the location of intermediate file regions is propagated from map tasks to reduce tasks. Therefore, for each completed map task, the master stores the locations and sizes of the $R$   intermediate file regions produced by the map task. Updates to this location and size information are received as map tasks are completed. The information is pushed incrementally to workers that have in-progress reduce tasks.

#### 3.3 Fault Tolerance

由于 Google MapReduce 很大程度上利用了由 Google File System 提供的分布式原子文件读写操作，所以 MapReduce 集群的容错机制实现相比之下便简洁很多，也主要集中在任务意外中断的恢复上。

##### woker failure

master **pings every worker periodically**. If no response from a worker timely, 

- the **master** marks the **worker** as failed. 
- Any map tasks completed by the worker are reset back to their initial idle state, therefore eligible for scheduling on other workers.
- Any map/reduce task in progress on a failed worker is also reset to idle and eligible for rescheduling.
- Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. 
- Completed reduce tasks **do not** need to be re-executed since their output is stored in a global file system.
- When a map task is executed first by $worker A$ and then later executed by $worker B$ (because $A$ failed), all workers executing reduce tasks are notified of the re-execution. Any reduce task that has not already read the data from $worker A$ will read the data from $worker B$.

MapReduce is resilient to large-scale worker failures. 

- For example, during one MapReduce operation, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several minutes. 

The MapReduce master simply re-executed the work done by the unreachable worker machines, and continued to make forward progress, eventually completing the MapReduce operation.

- 任何分配给该 Worker 的 Map 任务，无论是正在运行还是已经完成，都需要由 Master 重新分配给其他 Worker，因为该 Worker 不可用也意味着存储在该 Worker 本地磁盘上的中间结果也不可用了。Master 也会将这次重试通知给所有 Reducer，没能从原本的 Mapper 上完整获取中间结果的 Reducer 便会开始从新的 Mapper 上获取数据。


- 如果有 Reduce 任务分配给该 Worker，Master 则会选取其中尚未完成的 Reduce 任务分配给其他 Worker。鉴于 Google MapReduce 的结果是存储在 Google File System 上的，已完成的 Reduce 任务的结果的可用性由 Google File System 提供，因此 MapReduce Master 只需要处理未完成的 Reduce 任务即可。


##### master failure

Master 结点在运行时会周期性地将集群的当前状态作为**保存点（Checkpoint）**写入到磁盘中。Master 进程终止后，重新启动的 Master 进程即可利用存储在磁盘中的数据恢复到上一次保存点的状态。

> 但是，整个 MapReduce 集群中只会有一个 Master 结点，因此 Master 失效的情况并不多见。
>
> 因此，如果主节点失败，我们当前的实现将中止MapReduce计算。
>
> 客户端可以检查此条件，并根据需要重试MapReduce操作。

##### semantics in the presence of failure

当 User-supplied $map$ 和 $reduce$ 运算符是其输入值的确定性函数时，我们的分布式实现将生成与整个程序的非错误顺序执行所产生的输出相同的输出。

- 我们依靠 $map$ 的**原子提交**和 $reduce$ **任务输出**来实现此属性。每个正在进行的任务都会将其输出写入私有临时文件。
- $reduce$ 任务生成一个这样的文件，$map$ 任务生成 $R$ 这样的文件 (one per reduce task)。
  - $map$ task 完成后，$worker$ 会向 $master$ 发送一条消息，并在消息中包含 R 临时文件的名称。
  - 如果 $master$ 收到已完成的 $map$ task 的已完成消息，它将忽略该消息。否则，它将在 $master$ 的数据结构中记录 $R$ 文件的名称。
  - 当 $reduce$ 的任务完成时，$reduce$ worker 会 atomically 将其临时输出文件重命名为最终输出文件。
  - 如果在多台计算机上执行相同的 reduce 任务，则会对同一最终输出文件执行多个重命名调用。
  - 我们依靠**底层文件系统**提供的**原子重命名**操作来保证最终的文件系统状态只包含一次执行 $reduce$ 任务时生成的数据。

我们的绝大多数 $map$ 和 $reduce$ operators 都是 Deterministic 的，在这种情况下

- 我们的 semantics 等同于顺序执行的事实使得程序员很容易推断出他们的程序的 behavior。当 $map$ 和/或 $reduce$ operators 是非确定性的时，我们提供较弱但仍然合理的语义。
- 在存在 non-deterministic operators 的情况下，特定的 reduce 任务 R1 的输出等效于由 non-deterministic program 的 sequential execution 生成的 R1 的输出。

然而，不同 reduce 任务 R2 的输出可以对应于由 non-deterministic program 的 different sequential execution 产生的 R2 的输出。考虑 Map 任务 $M$ 和 Reduce 任务 $R_1$ 和 $R_2$:

- Let $e(Ri)$ be the execution of $R_i$ that committed (there is exactly one such execution).

- The weaker semantics arise because $e(R_1)$ may have read the output produced by one execution of $M$ , and $e(R_2)$ may have read the output produced by a different execution of $M$ .出现较弱的语义是因为 $e(R1)$ 可能已读取由 $M$ 的一个执行产生的输出，而 $e(R2)$ 可能已读取由 $M$ 的不同执行产生的输出。

#### 3.4 Locality

网络带宽资源相对稀缺，通过利用输入数据（GFS 管理）存储在组成集群的机器的本地磁盘上这一事实来节省网络带宽。<u>GFS 将每个文件划分为 `64MB` 的块，并在不同的机器上存储**每个块的多个副本**（typically 3 个副本）。</u>

MapReduce `master` 会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的计算机上调度 `map` 任务。如果做不到这一点，它会尝试在该任务的**输入数据的副本**附近调度 `map` 任务 

> (e.g., on a worker machine that is on the same network switch as the machine containing the data)。

当对集群中很大一部分 `wokers` 运行大型 MapReduce 操作时，<u>大多数</u>输入数据都是在<u>本地读取</u>的，并且<u>不消耗网络带宽</u>。

#### 3.5 Task Granularity

如上所述，我们将 `map` phase 细分为 $M$ 个部分，将 reduce phase 细分为 $R$ 个部分。

理想情况下，$M$ 和 $R$ 应比 worker machine 的数量大得多。让每个 woker 执行许多不同的任务可以改善动态负载平衡 (dynamic load balancing)，还可以在一个 worker fails 时加快恢复速度：它已完成的许多 map 任务可以被分布在所有其他 worker machine 上。

在我们的实现中，$M$ 和 $R$ 的大小是有实际限制的，因为 master 必须做出 $O(M+R)$ 调度决策，并将 $O(M*R)$ 状态保存在如上所述的内存中。（然而，constant factors for memory usage 很小：the $O(M*R)$  piece of the state consists of approximately one byte of data per map task/reduce task pair)

此外，$R$ 通常受到用户的约束，因为每个 reduce 任务的输出最终都位于单独的 output 文件中。在实践中，我们倾向于选择 $M$，以便每个单独的任务大约是 16 MB 到 64 MB 的输入数据（以便上面描述的 locality optimization 局部性优化是最有效的），并且我们使 $R$ 成为我们期望使用的 worker machine 数量的 small multiple。We often perform MapReduce computations with $M=200,000$ and $R=5,000$, using $2,000$ worker machines.

#### 3.6 Backup Tasks

#####  “straggler”：

- 一个需要非常长的时间才能完成计算中最后几个 map 或 reduce 任务之一的 machine。
- 延长 MapReduce 操作所花费的总时间的常见原因之一
- 出现原因：
  - 例如，磁盘损坏的计算机可能会遇到频繁的 correctable errors，将其读取性能从 $30MB/s$ 降低到 $1MB/s$。
  - 集群调度系统可能已经在机器上调度了其他任务，导致它由于 CPU，内存，本地磁盘或网络工作带宽的竞争而执行 MapReduce 代码的速度更慢。
  - machine initialization code 中的一个错误，导致 processor caches 被禁用：受影响的 machines上的计算速度减慢了一百倍以上。

##### Alleviation 通用机制：

当 MapReduce 操作接近完成时，master 会计划其余 *in-progress task* 的备份执行。每当 primary 或 backup executions 完成时，任务都会标记为已完成。

我们调整了此机制，使其通常将操作使用的计算资源增加不超过百分之几。我们发现这大大减少了完成大型MapReduce 操作的时间。

作为一个例子，the sort program described in Section 5.3 takes 44% longer to complete when the backup task mechanism is disabled.

### 4 Refinement

尽管简单编写 Map 和 Reduce 函数基本满足大多需求，但一些扩展很有用。

#### 4.1 Partitoning Function

MapReduce 的用户指定他们想要的减少任务/输出文件的数量 $(R)$。Data gets partitioned across these tasks using a partitioning function on the intermediate key. A default partitioning function is provided that uses hashing (e.g. “$hash(key) \mod R$”)。这往往会导致相当均衡的 partitions。但是，在某些情况下，some other function of the key 对数据进行分区是很有用的。例如，有时 the output keys 是 URLs，and 我们希望 a single host 的 all entries 最终都位于同一 output file 中。

为了支持这种情况，MapReduce 库的用户可以提供 special partitioning function。例如，使用 “$hash(Hostname(urlkey))\mod R$” 作为 partitioning 函数会导致来自同一主机的所有 URLs 最终都位于同一输出文件中。

#### 4.2 Ordering Guarantees

我们保证在给定分区中，中间键/值对按递增键顺序处理。这种排序保证使得每个分区生成一个排序的输出文件变得容易，当输出文件格式需要支持按键进行高效的随机访问查找时，或者输出的用户发现对数据进行排序很方便时，很有用。

#### 4.3 Combiner Function

在某些情况下，每个 map 任务生成的 intermediate keys 都存在显著的重复，并且用户指定的 `Reduce` 函数是可交换和关联的。一个例子是字数统计示例。由于词频倾向于遵循 Zipf 分布，因此每个映射任务将生成数百或数千条 $<the,1>$ 形式的记录。所有这些计数将通过网络发送到单个 `reduce` 任务，然后通过 `Reduce` 函数相加以生成一个数字。我们允许用户指定一个可选的 `Combiner` 函数，该函数在通过网络发送数据之前对数据进行部分合并。

`Combiner` 函数在每台执行 `map` 任务的计算机上执行。通常使用相同的代码来实现 `Combiner` 和 `reduce` 函数。`reduce` 函数和`Combiner` 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。`reduce` 函数的输出将写入最终的输出文件。`Combiner` 函数的输出将写入将发送到 `reduce` 任务的中间文件。

**Partial combining** 显著加快了某些类别的 MapReduce 操作。附录 A 包含一个使用 `combiner` 的示例。

#### 4.4 Input and Output Types

MapReduce 库支持以几种不同的格式读取输入数据。

- 例如，“text” 模式输入将每行视为键/值对：键是文件中的偏移量，值是行的内容。
- 另一种常见的格式是存储按键排序的键/值对序列。每个输入类型实现都知道如何将自身拆分为有意义的范围，以便作为单独的 `map` 任务进行处理（例如， text 模式的 range splitting 可确保 range splits 仅在行边界处发生）。

用户可以通过提供简单的 $x$ 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少量预定义输入类型中的一个。

A *reader* 不一定需要提供从文件读取的数据。例如，很容易定义一个从数据库或内存中映射的数据结构读取记录的读取器。以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。

#### 4.5 Side-effects

在某些情况下，MapReduce 的用户发现从他们的 `map` 和/或 `reduce` 运算符生成辅助文件作为附加输出很方便。我们依靠应用程序编写器使这种副作用具有原子性和幂等性。通常，应用程序会写入临时文件，并在完全生成此文件后以原子方式重命名该文件。

我们不为单个任务生成的多个输出文件的 atomic two-phase commits 提供支持。因此，生成具有跨文件一致性要求的多个输出文件的任务应具有确定性。这种限制在实践中从来都不是问题。

#### 4.6 Skipping Bad Records

有时，用户代码中存在一些错误，导致 `Map` 或 `Reduce` 函数在某些记录上确定性地崩溃。这样的错误会阻止MapReduce 操作完成。通常的操作过程是修复错误，但有时这是不可行的；也许该错误位于源代码不可用的第三方库中。此外，有时忽略一些记录是可以接受的，例如在对大型数据集进行统计分析时。我们提供了一种可选的执行模式，其中 MapReduce 库检测哪些记录会导致确定性崩溃并跳过这些记录以 make forward progress.

每个 `worker` 进程都安装一个 signal handler，用于捕获 segmentation violations 和 bus errors。在调用 User `Map` 或 `Reduce` 操作之前，MapReduce 库将参数的序列号存储在全局变量中。如果用户代码生成信号，信号处理程序会向MapReduce master 发送包含序列号的 “last gasp” UDP packet。当 mater 在特定记录上看到多个故障时，它指示在发出相应的 Map 或 Reduce 任务的下一次重新执行时应跳过该记录。

#### 4.7 Local Execution

`Map` 或 `Reduce` 函数中的调试问题可能很棘手，因为实际计算发生在分布式系统中，通常在数千台机器上，工作分配决策由主服务器动态做出。为了帮助促进调试、分析和小规模测试，我们开发了 MapReduce 库的替代实现，该库按顺序在本地计算机上执行 MapReduce 操作的所有工作。

向用户提供控件，以便可以将计算限制为特定的 `map` 任务。用户使用特殊的 flag invoke 他们的程序，然后可以轻松使用他们认为有用的任何调试或测试工具 (e.g. gdb).。

#### 4.8 Status Information

master 运行一个 internal HTTP 服务器，并导出一组状态页面供用户使用。状态页面 status pages 显示计算进度，例如已完成的任务数、正在进行的任务数、输入字节数、中间数据字节数、输出字节数、处理速率等。这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用此数据来预测计算将花费多长时间，以及是否应向计算添加更多资源。这些页面还可用于确定计算速度何时比预期的慢得多。

此外，top-level status page  显示哪些 `worker` 已 failed，以及他们在 fail 时正在处理的 `map/reduce` 任务。在尝试诊断用户代码中的 bug 时，此信息非常有用。

#### 4.9 Counters

MapReduce库提供了一个计数器工具来计算各种事件的发生次数。例如，用户代码可能希望计算处理的单词总数或索引的德语文档数等。

若要使用此工具，用户代码将创建一个命名的计数器对象，然后在 Map 和/或 Reduce 函数中相应地递增该计数器。例如：

```java
Counter* uppercase;
uppercase = GetCounter("uppercase");
map(String name, String contents):
  for each word w in contents:
    if (IsCapitalized(w)):
      uppercase->Increment();
    EmitIntermediate(w, "1");
```

来自单个工作计算机的计数器值定期传播到 master （piggybacked on the ping response）。主节点聚合成功 `map` 和 `reduce` 任务中的计数器值，并在 MapReduce 操作完成时将其返回到用户代码。当前计数器值也显示在主状态页面上，以便用户可以观察实时计算的进度。聚合计数器值时，master 会消除重复执行同一 map 或 reduce 任务的影响，以避免重复计数。（重复执行可能源于我们对备份任务的使用以及由于故障而重新执行任务。）

> 一些计数器值由 MapReduce 库自动维护，例如处理的输入键/值对的数量和生成的输出键/值对的数量。

用户发现计数器工具对于检查 MapReduce 操作行为的健全性非常有用。

> 例如，在某些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者处理的德语文档的分数在处理的文档总数的某个可容忍范围内。

### 5 Performance

MapReduce 在大型机器集群上运行的两次计算上的性能。

> 代表了MapReduce用户编写的真实程序的大型子集 —— 
>
> - 一类程序将数据从一种表示形式洗牌到另一种表示形式，
>
> - 另一类从大型数据集中提取少量有趣的数据。

- 一个计算搜索大约一TB的数据，寻找特定的模式
- 另一个计算对大约 1TB 的数据进行排序

#### 5.1 Cluster Configuration

所有程序都在由大约 1800 台计算机组成的集群上执行。每个机器都有两个启用了 Hyper-Threading 的 2GHz 英特尔至强处理器，4GB 内存，两个 160GB IDE 磁盘和一个千兆以太网link。这些机器被安排在一个两级树形交换网络中，根目录下可用的总带宽约为 100-200 Gbps。所有机器都在同一托管设施中，因此任何一对机器之间的往返时间不到一毫秒。在 4GB 内存中，大约 1-1.5GB 是由群集上运行的其他任务保留的。这些程序在周末下午执行，当时CPU，磁盘和网络大多处于空闲状态。

#### 5.2 Grep

`grep` 程序扫描 1010 条 $100-byte$ 记录，搜索相对罕见的三字符模式（该模式出现在 $92,337$ 条记录中）。输入被拆分为大约 64MB 的片段 $(M = 15000)$，全部输出被放置在一个文件 $(R=1)$  中。

![figure2](figure2.png)

图显示了计算随时间推移的进度。Y 轴显示扫描输入数据的速率。随着更多的机器被分配到这个 MapReduce 计算，速率逐渐回升，当分配了 1764 个 worker 时，速率的峰值超过 30 GB / s。当 map 任务完成时，速率开始下降，并在计算中大约 80 秒处达到零。整个计算从开始到结束大约需要 150 秒。这包括大约一分钟的启动开销。开销是由于程序传播到所有 woker 机器，以及与 GFS 交互以打开 1000 个输入文件集和获取局部性优化所需信息的延迟。

#### 5.3 Sort

排序程序对 1010 条 100-bytes 记录（大约 1TB 的数据）进行排序。该程序以 TeraSort 基准测试为蓝本。

排序程序由少于 50 行的用户代码组成。三行 Map 函数从文本行中提取 10-bytes 排序键，并将该键和原始文本行作为中间键/值对发出。我们使用内置的 identifty 函数作为 reduce 运算符。此函数将中间键/值对作为输出键/值对传递，保持不变。最终的排序输出被写入一组双向复制的 GFS 文件（i.e. 2TB 被写入为程序的输出）。

与之前一样，输入数据被拆分为 64MB 的片段 $(M=15000)$。我们将排序后的输出划分为 4000 个文件 $(R=4000)$。分区函数使用密钥的初始字节将其隔离为 $R$ 的片段之一。

我们针对此基准测试的分区功能内置了密钥分布知识。在常规排序程序中，我们将添加一个 pre-pass MapReduce 操作，该操作将收集键的样本，并使用 samples of the keys 的分布来计算最终排序传递的 split-points。

![figure3](figure3.png)

Figure 3 (a) shows the progress of a normal execution of the sort program. The top-left graph shows the rate at which input is read. The rate peaks at about 13 GB/s and dies off fairly quickly since all map tasks finish be- fore 200 seconds have elapsed. Note that the input rate is less than for grep. This is because the sort map tasks spend about half their time and I/O bandwidth writing in- termediate output to their local disks. The corresponding intermediate output for grep had negligible size.

The middle-left graph shows the rate at which data is sent over the network from the map tasks to the re- duce tasks. This shuffling starts as soon as the first map task completes. The first hump in the graph is for the first batch of approximately 1700 reduce tasks (the entire MapReduce was assigned about 1700 machines, and each machine executes at most one reduce task at a time). Roughly 300 seconds into the computation, some of these first batch of reduce tasks finish and we start shuffling data for the remaining reduce tasks. All of the shuffling is done about 600 seconds into the computation.

The bottom-left graph shows the rate at which sorted data is written to the final output files by the reduce tasks. There is a delay between the end of the first shuffling pe- riod and the start of the writing period because the ma- chines are busy sorting the intermediate data. The writes continue at a rate of about 2-4 GB/s for a while. All of the writes finish about 850 seconds into the computation. Including startup overhead, the entire computation takes 891 seconds. This is similar to the current best reported result of 1057 seconds for the TeraSort benchmark [18].

A few things to note: the input rate is higher than the shuffle rate and the output rate because of our locality optimization – most data is read from a local disk and bypasses our relatively bandwidth constrained network. The shuffle rate is higher than the output rate because the output phase writes two copies of the sorted data (we make two replicas of the output for reliability and avail- ability reasons). We write two replicas because that is the mechanism for reliability and availability provided by our underlying file system. Network bandwidth re- quirements for writing data would be reduced if the un- derlying file system used erasure coding [14] rather than replication.

#### 5.4 Effect of Backup Tasks

In Figure 3(b), we show an execution of the sort pro- gram with backup tasks disabled. The execution flow is similar to that shown in Figure 3 (a), except that there is a very long tail where hardly any write activity occurs. After 960 seconds, all except 5 of the reduce tasks are completed. However these last few stragglers don’t fin- ish until 300 seconds later. The entire computation takes 1283 seconds, an increase of 44% in elapsed time.

#### 5.5 Machine Failures

In Figure 3 (c), we show an execution of the sort program where we intentionally killed 200 out of 1746 worker processes several minutes into the computation. The underlying cluster scheduler immediately restarted new worker processes on these machines (since only the pro- cesses were killed, the machines were still functioning properly).

The worker deaths show up as a negative input rate since some previously completed map work disappears (since the corresponding map workers were killed) and needs to be redone. The re-execution of this map work happens relatively quickly. The entire computation fin- ishes in 933 seconds including startup overhead (just an increase of 5% over the normal execution time).

### 6 Experience

We wrote the first version of the MapReduce library in February of 2003, and made significant enhancements to it in August of 2003, including the locality optimization, dynamic load balancing of task execution across worker machines, etc. Since that time, we have been pleasantly surprised at how broadly applicable the MapReduce li- brary has been for the kinds of problems we work on. It has been used across a wide range of domains within Google, including:

- • large-scale machine learning problems,
- • clustering problems for the Google News and Froogle products,
- • extraction of data used to produce reports of popular queries (e.g. Google Zeitgeist),
- • extractionofpropertiesofwebpagesfornewexper- iments and products (e.g. extraction of geographi- cal locations from a large corpus of web pages for localized search), and
- • large-scale graph computations.

![figure4](figure4.png)

Figure 4 shows the significant growth in the number of separate MapReduce programs checked into our primary source code management system over time, from 0 in early 2003 to almost 900 separate instances as of late September 2004. MapReduce has been so successful be- cause it makes it possible to write a simple program and run it efficiently on a thousand machines in the course of half an hour, greatly speeding up the development and prototyping cycle. Furthermore, it allows programmers who have no experience with distributed and/or parallel systems to exploit large amounts of resources easily.

At the end of each job, the MapReduce library logs statistics about the computational resources used by the job. In Table 1, we show some statistics for a subset of MapReduce jobs run at Google in August 2004.

![table1](table1.png)

#### 6.1 Large-Scale Indexing

One of our most significant uses of MapReduce to date has been a complete rewrite of the production index-ing system that produces the data structures used for the Google web search service. The indexing system takes as input a large set of documents that have been retrieved by our crawling system, stored as a set of GFS files. The raw contents for these documents are more than 20 ter- abytes of data. The indexing process runs as a sequence of five to ten MapReduce operations. Using MapReduce (instead of the ad-hoc distributed passes in the prior version of the indexing system) has provided several benefits:

- • The indexing code is simpler, smaller, and easier to understand, because the code that deals with fault tolerance, distribution and parallelization is hidden within the MapReduce library. For example, the size of one phase of the computation dropped from approximately 3800 lines of C++ code to approx- imately 700 lines when expressed using MapReduce.
- • The performance of the MapReduce library is good enough that we can keep conceptually unrelated computations separate, instead of mixing them to- gether to avoid extra passes over the data. This makes it easy to change the indexing process. For example, one change that took a few months to make in our old indexing system took only a few days to implement in the new system.
- • The indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapRe- duce library without operator intervention. Further- more, it is easy to improve the performance of the indexing process by adding new machines to the in- dexing cluster.

### 7 Related Work

Many systems have provided restricted programming models and used the restrictions to parallelize the com- putation automatically. For example, an associative func- tion can be computed over all prefixes of an N element array in log N time on N processors using parallel prefix computations [6, 9, 13]. MapReduce can be considered a simplification and distillation of some of these models based on our experience with large real-world compu- tations. More significantly, we provide a fault-tolerant implementation that scales to thousands of processors. In contrast, most of the parallel processing systems have only been implemented on smaller scales and leave the details of handling machine failures to the programmer.

Bulk Synchronous Programming [17] and some MPI primitives [11] provide higher-level abstractions that make it easier for programmers to write parallel pro- grams. A key difference between these systems and MapReduce is that MapReduce exploits a restricted pro- gramming model to parallelize the user program auto- matically and to provide transparent fault-tolerance.

Our locality optimization draws its inspiration from techniques such as active disks [12, 15], where compu- tation is pushed into processing elements that are close to local disks, to reduce the amount of data sent across I/O subsystems or the network. We run on commodity processors to which a small number of disks are directly connected instead of running directly on disk controller processors, but the general approach is similar.

Our backup task mechanism is similar to the eager scheduling mechanism employed in the Charlotte Sys- tem [3]. One of the shortcomings of simple eager scheduling is that if a given task causes repeated failures, the entire computation fails to complete. We fix some in- stances of this problem with our mechanism for skipping bad records.

The MapReduce implementation relies on an in-house cluster management system that is responsible for dis- tributing and running user tasks on a large collection of shared machines. Though not the focus of this paper, the cluster management system is similar in spirit to other systems such as Condor [16].

The sorting facility that is a part of the MapReduce library is similar in operation to NOW-Sort [1]. Source machines (map workers) partition the data to be sorted and send it to one of R reduce workers. Each reduce worker sorts its data locally (in memory if possible). Of course NOW-Sort does not have the user-definable Map and Reduce functions that make our library widely applicable.

River [2] provides a programming model where pro- cesses communicate with each other by sending data over distributed queues. Like MapReduce, the River system tries to provide good average case performance even in the presence of non-uniformities introduced by heterogeneous hardware or system perturbations. River achieves this by careful scheduling of disk and network transfers to achieve balanced completion times. MapRe- duce has a different approach. By restricting the programming model, the MapReduce framework is able to partition the problem into a large number of fine- grained tasks. These tasks are dynamically scheduled on available workers so that faster workers process more tasks. The restricted programming model also allows us to schedule redundant executions of tasks near the end of the job which greatly reduces completion time in the presence of non-uniformities (such as slow or stuck workers).

BAD-FS [5] has a very different programming model from MapReduce, and unlike MapReduce, is targeted to the execution of jobs across a wide-area network. How- ever, there are two fundamental similarities. (1) Both systems use redundant execution to recover from data loss caused by failures. (2) Both use locality-aware scheduling to reduce the amount of data sent across con- gested network links.

TACC [7] is a system designed to simplify con- struction of highly-available networked services. Like MapReduce, it relies on re-execution as a mechanism for implementing fault-tolerance.

### 8 Conclusions

The MapReduce programming model has been success- fully used at Google for many different purposes. We attribute this success to several reasons. First, the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality opti- mization, and load balancing. Second, a large variety of problems are easily expressible as MapReduce com- putations. For example, MapReduce is used for the gen- eration of data for Google’s production web search ser- vice, for sorting, for data mining, for machine learning, and many other systems. Third, we have developed an implementation of MapReduce that scales to large clus- ters of machines comprising thousands of machines. The implementation makes efficient use of these machine re- sources and therefore is suitable for use on many of the large computational problems encountered at Google.

We have learned several things from this work. First, restricting the programming model makes it easy to par- allelize and distribute computations and to make such computations fault-tolerant. Second, network bandwidth is a scarce resource. A number of optimizations in our system are therefore targeted at reducing the amount of data sent across the network: the locality optimization al- lows us to read data from local disks, and writing a single copy of the intermediate data to local disk saves network bandwidth. Third, redundant execution can be used to reduce the impact of slow machines, and to handle ma- chine failures and data loss.

### Acknowledgements

Josh Levenberg has been instrumental in revising and extending the user-level MapReduce API with a num- ber of new features based on his experience with using MapReduce and other people’s suggestions for enhance- ments. MapReduce reads its input from and writes its output to the Google File System [8]. We would like to thank Mohit Aron, Howard Gobioff, Markus Gutschke, David Kramer, Shun-Tak Leung, and Josh Redstone for their work in developing GFS. We would also like to thank Percy Liang and Olcan Sercinoglu for their work in developing the cluster management system used by MapReduce. Mike Burrows, Wilson Hsieh, Josh Leven- berg, Sharon Perl, Rob Pike, and Debby Wallach pro- vided helpful comments on earlier drafts of this pa- per. The anonymous OSDI reviewers, and our shepherd, Eric Brewer, provided many useful suggestions of areas where the paper could be improved. Finally, we thank all the users of MapReduce within Google’s engineering or- ganization for providing helpful feedback, suggestions, and bug reports.

## Lab
